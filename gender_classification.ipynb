{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit user gender classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['grid', 'science', 'notebook', 'mylegend'])\n",
    "\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(load_test: bool = False) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = pd.read_csv(f'{data_dir}/train_data.csv')\n",
    "    target = pd.read_csv(f'{data_dir}/train_target.csv')\n",
    "    if load_test:\n",
    "        test_data = pd.read_csv(f'{data_dir}/test_data.csv')\n",
    "    else:\n",
    "        test_data = pd.DataFrame()\n",
    "    return train_data, target, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 5000\n"
     ]
    }
   ],
   "source": [
    "train_data, target, _ = load_data()\n",
    "\n",
    "print(f\"Number of authors in training set: {train_data[\"author\"].unique().shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subreddit_idx(data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Map every subreddit to a unique integer.\"\"\"\n",
    "    subreddits = data[\"subreddit\"].unique()\n",
    "    return pd.Series(index=subreddits, data=np.arange(len(subreddits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subreddits(\n",
    "    author_data: pd.DataFrame,\n",
    "    subreddit_idx: pd.Series,\n",
    ") -> sparse.csr_array:\n",
    "    \"\"\"\n",
    "    This function converts all the subreddits the author has posted in into a sparse\n",
    "    array of length N (where N is the number of subreddits in the dataset) with 1s in\n",
    "    the indexes of the subreddits the author has posted in.\n",
    "    \"\"\"\n",
    "    user_subreddits = author_data[\"subreddit\"].to_numpy()\n",
    "\n",
    "    # idxs is an array with the indexes of the subreddits in subreddits_idx\n",
    "    idxs = subreddit_idx.loc[user_subreddits].to_numpy()\n",
    "\n",
    "    # create a sparse array indicating the subreddits the author has posted in\n",
    "    v = sparse.dok_array(shape=(1, len(subreddit_idx)))  # dok = dictionary of keys\n",
    "    for idx in idxs:\n",
    "        v[0, idx] = 1\n",
    "    return v.tocsr()  # convert to compressed sparse row format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(author_data: pd.DataFrame) -> str:\n",
    "    \"\"\"Returns all the posts of an author as a single string.\"\"\"\n",
    "    group_text = author_data[\"body\"].astype(str).to_numpy()\n",
    "    return \" \".join(group_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(vectorizer: TfidfVectorizer, text: str) -> sparse.csr_array:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "    data: pd.DataFrame,\n",
    "    subreddit_idx: pd.Series,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    *,\n",
    "    target: pd.DataFrame | None = None,\n",
    ") -> tuple[sparse.csr_matrix, pd.Series] | sparse.csr_matrix:\n",
    "    \"\"\"Extract features from the data.\"\"\"\n",
    "\n",
    "    subreddits_dict: dict[str, sparse.csr_array] = {}\n",
    "    for author, group in data.groupby(\"author\"):\n",
    "        subreddits_dict[author] = extract_subreddits(group, subreddit_idx)\n",
    "\n",
    "    # Generate a sparse matrix with the labelled authors as rows and the subreddits they\n",
    "    # have posted in as columns\n",
    "    subreddits_matrix: sparse.csr_matrix = sparse.vstack(\n",
    "        [subreddits_dict[author] for author in target[\"author\"]]\n",
    "    )\n",
    "\n",
    "    text_dict: dict[str, str] = {}\n",
    "    for author, group in data.groupby(\"author\"):\n",
    "        text_dict[author] = extract_text(group)\n",
    "\n",
    "    author_text: list[str] = [text_dict[author] for author in target[\"author\"]]\n",
    "    text_features = vectorize_text(vectorizer, author_text)\n",
    "\n",
    "    # print(type(text_features))\n",
    "\n",
    "    X = sparse.hstack([subreddits_matrix, text_features])\n",
    "\n",
    "    if target is None:\n",
    "        return X\n",
    "    else:\n",
    "        y: pd.Series = target[\"gender\"]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_idx = create_subreddit_idx(train_data)\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, stop_words=\"english\", max_features=10000)  #Â max_features needs to be tuned !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = extract_features(X_train, subreddit_idx, vectorizer, target=y_train)\n",
    "X_val, y_val = extract_features(X_val, subreddit_idx, vectorizer, target=y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit_data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
