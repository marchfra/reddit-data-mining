{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit user gender classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['grid', 'science', 'notebook', 'mylegend'])\n",
    "\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(load_test: bool = False) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = pd.read_csv(f'{data_dir}/train_data.csv')\n",
    "    target = pd.read_csv(f'{data_dir}/train_target.csv')\n",
    "    if load_test:\n",
    "        test_data = pd.read_csv(f'{data_dir}/test_data.csv')\n",
    "    else:\n",
    "        test_data = pd.DataFrame()\n",
    "    return train_data, target, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, target, test_data = load_data(load_test=True)\n",
    "\n",
    "print(f\"Number of authors in training set: {train_data[\"author\"].unique().shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subreddit_idx(data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Map every subreddit to a unique integer.\"\"\"\n",
    "    subreddits = data[\"subreddit\"].unique()\n",
    "    return pd.Series(index=subreddits, data=np.arange(len(subreddits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subreddits(\n",
    "    author_data: pd.DataFrame,\n",
    "    subreddit_idx: pd.Series,\n",
    ") -> sp.csr_array:\n",
    "    \"\"\"\n",
    "    This function converts all the subreddits the author has posted in into a sparse\n",
    "    array of length N (where N is the number of subreddits in the dataset) with 1s in\n",
    "    the indexes of the subreddits the author has posted in.\n",
    "    \"\"\"\n",
    "    user_subs = author_data[\"subreddit\"]\n",
    "    subs_in_idx = user_subs.isin(subreddit_idx.index)\n",
    "    user_subs = user_subs[subs_in_idx].to_numpy()\n",
    "\n",
    "    # idxs is an array with the indexes of the subreddits in subreddits_idx\n",
    "    idxs = subreddit_idx.loc[user_subs].to_numpy()\n",
    "\n",
    "    # create a sparse array indicating the subreddits the author has posted in\n",
    "    v = sp.dok_array((1, len(subreddit_idx)))  # dok = dictionary of keys\n",
    "    for idx in idxs:\n",
    "        v[0, idx] = 1\n",
    "    return v.tocsr()  # convert to compressed sparse row format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(author_data: pd.DataFrame) -> str:\n",
    "    \"\"\"Returns all the posts of an author as a single string.\"\"\"\n",
    "    group_text = author_data[\"body\"].astype(str).to_numpy()\n",
    "    return \" \".join(group_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    text: list[str],\n",
    "    data_is_test: bool,\n",
    ") -> sp.csr_array:\n",
    "    \"\"\"\n",
    "    This function vectorizes the text of an author using the provided vectorizer.\n",
    "    If the data is test data, the vectorizer is only transformed, otherwise it is fit\n",
    "    and transformed.\n",
    "    \"\"\"\n",
    "    if data_is_test:\n",
    "        return vectorizer.transform(text)\n",
    "    else:\n",
    "        return vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "    data: pd.DataFrame,\n",
    "    subreddit_idx: pd.Series,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    *,\n",
    "    target: pd.DataFrame | None = None,\n",
    ") -> tuple[sp.csr_matrix, pd.Series] | sp.csr_matrix:\n",
    "    \"\"\"Extract features from the data.\"\"\"\n",
    "\n",
    "    data_is_test = True if target is None else False\n",
    "\n",
    "    subs_dict: dict[str, sp.csr_array] = {}\n",
    "    for author, group in data.groupby(\"author\"):\n",
    "        subs_dict[author] = extract_subreddits(group, subreddit_idx)\n",
    "\n",
    "    if data_is_test:\n",
    "        authors = data[\"author\"].unique()\n",
    "    else:\n",
    "        authors = target[\"author\"]\n",
    "\n",
    "    # Generate a sparse matrix with the authors as rows\n",
    "    # and the subreddits they have posted in as columns\n",
    "    subs_matrix: sp.csr_matrix = sp.vstack([subs_dict[author] for author in authors])\n",
    "\n",
    "    text_dict: dict[str, str] = {}\n",
    "    for author, group in data.groupby(\"author\"):\n",
    "        text_dict[author] = extract_text(group)\n",
    "\n",
    "    author_text: list[str] = [text_dict[author] for author in authors]\n",
    "    text_features = vectorize_text(vectorizer, author_text, data_is_test)\n",
    "\n",
    "    # print(type(text_features))\n",
    "\n",
    "    X = sp.hstack([subs_matrix, text_features])\n",
    "\n",
    "    if data_is_test:\n",
    "        return X\n",
    "    else:\n",
    "        y: pd.Series = target[\"gender\"]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_idx = create_subreddit_idx(train_data)\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, stop_words=\"english\", max_features=10000)  #Â max_features needs to be tuned !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extract_features(train_data, subreddit_idx, vectorizer, target=target)\n",
    "# X_test = extract_features(test_data, subreddit_idx, vectorizer)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a set of models to try on the dataset. Then, for each model, perform hyperparameters tuning using `GridSearchCV`. Finally, pick the best model overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "\n",
    "classifiers = {\"LogReg\": LogReg(),\n",
    "               \"SVM\": SVC(probability=True),\n",
    "               \"KNN\": KNN(),\n",
    "               \"Naive Bayes\": NB(),\n",
    "               \"Decision Tree\": DT(),\n",
    "               \"Gradient Boosting\": GBC(),\n",
    "               \"Random Forest\": RF(),\n",
    "               \"MultiLayer Perceptron\": MLP(),\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"{name} -- parameters: {clf.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = [{'C': np.logspace(0, 3)},\n",
    "               {'C': np.logspace(0, 3), \"kernel\": ['linear', 'poly', 'rbf'], \"degree\": np.arange(2, 5), \"gamma\": ['scale', 'auto', 1.0e-3, 1.0e-4]},\n",
    "               {'n_neighbors': np.arange(1, 10), \"weights\": ['uniform', 'distance']},\n",
    "               {},\n",
    "               {'max_depth': np.arange(1, 10), \"min_samples_split\": np.arange(2, 10)},\n",
    "               {'n_estimators': np.arange(1, 100, 10), \"learning_rate\": np.logspace(-3, 0), \"max_depth\": np.arange(1, 5)},\n",
    "               {'n_estimators': np.arange(1, 100, 10)},\n",
    "               {'hidden_layer_sizes': [(100,), (200,), (300,)], \"activation\": [\"logistic\", \"tanh\", \"relu\"], \"solver\": [\"adam\", \"sgd\"], \"alpha\": np.logspace(-3, 0), \"learning_rate\": [\"constant\", \"adaptive\"], \"learning_rate_init\": np.logspace(-3, 0), \"early_stopping\": [True, False]},\n",
    "            ]\n",
    "\n",
    "best_clfs = {}\n",
    "best_pars = {}\n",
    "for name, clf, param_grid in zip(classifiers.items(), param_grids):\n",
    "    search = GridSearchCV(clf, param_grid, cv=5, scoring='roc_auc')\n",
    "    search.fit(X, y)\n",
    "    best_clfs[name] = search.best_estimator_\n",
    "    best_pars[name] = search.best_params_\n",
    "\n",
    "best_scores = {}\n",
    "for name, clf in best_clfs.items():\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc')\n",
    "    best_scores[name] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classifier           Score\")\n",
    "for name, score in best_scores.items():\n",
    "    print(f\"{name:21}{score.mean():.6f} +/- {score.std():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit_data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
