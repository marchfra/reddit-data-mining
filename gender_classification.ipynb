{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit user gender classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta as td\n",
    "from time import time\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import GridSearchCV, HalvingGridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['grid', 'science', 'notebook', 'mylegend'])\n",
    "\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(load_test: bool) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    train_data = pd.read_csv(f'{data_dir}/train_data.csv')\n",
    "    target = pd.read_csv(f'{data_dir}/train_target.csv')\n",
    "    if load_test:\n",
    "        test_data = pd.read_csv(f'{data_dir}/test_data.csv')\n",
    "    else:\n",
    "        test_data = pd.DataFrame()\n",
    "    return train_data, target, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, target, test_data = load_data(load_test=True)\n",
    "\n",
    "print(f\"Number of authors in training set: {train_data[\"author\"].unique().shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subreddit_idx(data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Map every subreddit to a unique integer.\"\"\"\n",
    "    subreddits = data[\"subreddit\"].unique()\n",
    "    return pd.Series(index=subreddits, data=np.arange(len(subreddits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subreddits(\n",
    "    author_data: pd.DataFrame,\n",
    "    subreddit_idx: pd.Series,\n",
    ") -> sp.csr_array:\n",
    "    \"\"\"\n",
    "    This function converts all the subreddits the author has posted in into a sparse\n",
    "    array of length N (where N is the number of subreddits in the dataset) with 1s in\n",
    "    the indexes of the subreddits the author has posted in.\n",
    "    \"\"\"\n",
    "    user_subs = author_data[\"subreddit\"]\n",
    "    subs_in_idx = user_subs.isin(subreddit_idx.index)\n",
    "    user_subs = user_subs[subs_in_idx].to_numpy()\n",
    "\n",
    "    # idxs is an array with the indexes of the subreddits in subreddits_idx\n",
    "    idxs = subreddit_idx.loc[user_subs].to_numpy()\n",
    "\n",
    "    # create a sparse array indicating the subreddits the author has posted in\n",
    "    v = sp.dok_array((1, len(subreddit_idx)))  # dok = dictionary of keys\n",
    "    for idx in idxs:\n",
    "        v[0, idx] = 1\n",
    "    return v.tocsr()  # convert to compressed sparse row format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(author_data: pd.DataFrame) -> str:\n",
    "    \"\"\"Returns all the posts of an author as a single string.\"\"\"\n",
    "    group_text = author_data[\"body\"].astype(str).to_numpy()\n",
    "    return \" \".join(group_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    text: list[str],\n",
    "    data_is_test: bool,\n",
    ") -> sp.csr_array:\n",
    "    \"\"\"\n",
    "    This function vectorizes the text of an author using the provided vectorizer.\n",
    "    If the data is test data, the vectorizer is only transformed, otherwise it is fit\n",
    "    and transformed.\n",
    "    \"\"\"\n",
    "    if data_is_test:\n",
    "        return vectorizer.transform(text)\n",
    "    else:\n",
    "        return vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "    data: pd.DataFrame,\n",
    "    subreddit_idx: pd.Series,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    *,\n",
    "    target: pd.DataFrame | None = None,\n",
    ") -> tuple[sp.csr_matrix, pd.Series] | sp.csr_matrix:\n",
    "    \"\"\"Extract features from the data.\"\"\"\n",
    "\n",
    "    data_is_test = True if target is None else False\n",
    "\n",
    "    subs_dict: dict[str, sp.csr_array] = {}\n",
    "    for author, group in data.groupby(\"author\"):\n",
    "        subs_dict[author] = extract_subreddits(group, subreddit_idx)\n",
    "\n",
    "    if data_is_test:\n",
    "        authors = data[\"author\"].unique()\n",
    "    else:\n",
    "        authors = target[\"author\"]\n",
    "\n",
    "    # Generate a sparse matrix with the authors as rows\n",
    "    # and the subreddits they have posted in as columns\n",
    "    subs_matrix: sp.csr_matrix = sp.vstack([subs_dict[author] for author in authors])\n",
    "\n",
    "    text_dict: dict[str, str] = {}\n",
    "    for author, group in data.groupby(\"author\"):\n",
    "        text_dict[author] = extract_text(group)\n",
    "\n",
    "    author_text: list[str] = [text_dict[author] for author in authors]\n",
    "    text_features = vectorize_text(vectorizer, author_text, data_is_test)\n",
    "\n",
    "    # print(type(text_features))\n",
    "\n",
    "    X = sp.hstack([subs_matrix, text_features])\n",
    "\n",
    "    if data_is_test:\n",
    "        return X\n",
    "    else:\n",
    "        y: pd.Series = target[\"gender\"]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_idx = create_subreddit_idx(train_data)\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, stop_words=\"english\", max_features=10000)  # max_features needs to be tuned !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extract_features(train_data, subreddit_idx, vectorizer, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(target[\"gender\"], bins=2, color=\"skyblue\", edgecolor=\"black\", linewidth=1.2, align=\"mid\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is strongly unbalanced towards the male class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "Define a set of models to try on the dataset. Then, for each model, perform hyperparameters tuning using `GridSearchCV`. Finally, pick the best model overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "# from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "\n",
    "classifiers = {\"LogReg\": LogReg(n_jobs=-1),\n",
    "               \"KNN\": KNN(n_jobs=-1),\n",
    "            #    \"Naive Bayes\": NB(),\n",
    "               \"Random Forest\": RF(n_jobs=-1),\n",
    "               \"Decision Tree\": DT(),\n",
    "               \"Gradient Boosting\": GBC(),\n",
    "               \"MultiLayer Perceptron\": MLP(solver=\"adam\"),\n",
    "               \"SVM\": SVC(probability=True),\n",
    "}\n",
    "\n",
    "# for name, clf in classifiers.items():\n",
    "#     print(f\"{name} -- parameters: {clf.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = [\n",
    "    {\"C\": np.logspace(-2, 5)},  # LogReg\n",
    "    {\"n_neighbors\": np.arange(1, 20), \"weights\": [\"uniform\", \"distance\"]},  # KNN\n",
    "    # {},  # Naive Bayes\n",
    "    {\"n_estimators\": np.arange(1, 100, 5)},  # Random Forest\n",
    "    {\"max_depth\": np.arange(1, 10), \"min_samples_split\": np.arange(2, 5)},  # Decision Tree\n",
    "    {  # Gradient Boosting\n",
    "        \"n_estimators\": np.arange(1, 20, 4),\n",
    "        \"learning_rate\": np.logspace(-3, 0, num=4, endpoint=True),\n",
    "        \"max_depth\": np.arange(1, 5),\n",
    "    },\n",
    "    {  # MultiLayer Perceptron\n",
    "        \"hidden_layer_sizes\": [(20,), (10, 5), (10, 5, 2)],\n",
    "        \"activation\": [\"logistic\", \"relu\"],\n",
    "        \"alpha\": np.logspace(-4, -2, num=3, endpoint=True),\n",
    "        \"learning_rate\": [\"constant\", \"adaptive\"],\n",
    "        \"learning_rate_init\": np.logspace(-3, 0, num=4, endpoint=True),\n",
    "        \"early_stopping\": [True, False],\n",
    "    },\n",
    "    [  # SVM\n",
    "        {\n",
    "            \"C\": np.logspace(0, 2, num=3, endpoint=True),\n",
    "            \"kernel\": [\"linear\"],\n",
    "        },\n",
    "        {\n",
    "            \"C\": np.logspace(0, 2, num=3, endpoint=True),\n",
    "            \"kernel\": [\"rbf\"],\n",
    "            \"gamma\": [\"scale\", \"auto\"],\n",
    "        },\n",
    "        {\n",
    "            \"C\": np.logspace(0, 2, num=3, endpoint=True),\n",
    "            \"kernel\": [\"poly\"],\n",
    "            \"degree\": np.arange(2, 4),\n",
    "            \"gamma\": [\"scale\", \"auto\"],\n",
    "        },\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grids = [{}] * len(classifiers)\n",
    "\n",
    "best_clfs = {}\n",
    "best_pars = {}\n",
    "for (name, clf), param_grid in zip(classifiers.items(), param_grids):\n",
    "    print(f\"Training {name:21} -- \", end=\"\")\n",
    "    search = HalvingGridSearchCV(clf, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    t_start = time()\n",
    "    search.fit(X, y)\n",
    "    t_end = time()\n",
    "    best_clfs[name] = search.best_estimator_\n",
    "    best_pars[name] = search.best_params_\n",
    "    print(f\"score = {search.best_score_:.3f}, time = {td(seconds=t_end - t_start)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores_cv = {}\n",
    "for name, clf in best_clfs.items():\n",
    "    print(f\"Scoring {name:21}\", end=\" \")\n",
    "    t_start = time()\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    t_end = time()\n",
    "    best_scores_cv[name] = scores\n",
    "    print(f\"time = {t_end - t_start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classifier            Score\")\n",
    "for name, scores in best_scores_cv.items():\n",
    "    print(f\"{name:21} {scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores = {name: (scores.mean(), scores.std()) for name, scores in best_scores_cv.items()}\n",
    "best_clf_name = max(best_scores, key=lambda k: best_scores[k][0])\n",
    "print(f\"The best classifier is {best_clf_name}\")\n",
    "print(f\"score = {best_scores[best_clf_name][0]:.3f} +/- {best_scores[best_clf_name][1]:.3f}\")\n",
    "print(f\"parameters = {best_pars[best_clf_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = best_clfs[best_clf_name]\n",
    "joblib.dump(best_clf, f\"{data_dir}/best_clf_coarse_tuned.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fine tune the parameters of this model to see if the score can be improved further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\"C\": np.logspace(-1, 1)},\n",
    "    {\"C\": np.logspace(0, 1)},\n",
    "    {\"C\": np.logspace(0, 0.5)},\n",
    "]\n",
    "search = GridSearchCV(best_clf, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "search.fit(X, y)\n",
    "best_clf_fine_tuned = search.best_estimator_\n",
    "best_par_fine_tuned = search.best_params_\n",
    "print(f\"parameters = {best_par_fine_tuned}\")\n",
    "print(f\"score = {search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = extract_features(test_data, subreddit_idx, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.DataFrame({\"author\": test_data.author.unique(), \"gender\": y_pred})\n",
    "solution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to [Kaggle](https://www.kaggle.com/competitions/datamining2024/overview), click \"Submit Prediction\" and upload the file \"submission.csv\" to see the test score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit_data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
